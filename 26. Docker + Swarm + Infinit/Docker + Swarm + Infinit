////====================================================////
////                                                    ////
////               Docker + Swarm + Infinit             ////
////                                                    ////
////====================================================////
////                      ////
////         Ссылки       ////
////                      ////
////======================////


  // Docker //
  //--------//

    # Официальный сайт Docker
        https://www.docker.com/

    # Официальный репозиторий Docker-образов
        https://hub.docker.com/

    # Полное практическое руководство по Docker: с нуля до кластера на AWS
        https://habrahabr.ru/post/310460/#awsecs


  // Docker Swarm //
  //--------------//

    # Официальная документация Docker Swarm
        https://docs.docker.com/engine/swarm/


  // Infinit //
  //---------//

    # Официальный сайт Infinit
        http://infinit.sh/

    # Persistent Storage for Docker
        https://infinit.sh/docker

    # Docker Volume Plugin
        https://infinit.sh/documentation/docker/volume-plugin

    # Examples of Deployments
        https://infinit.sh/documentation/deployments

    # Playing with Infinit & Docker
        https://media-glass.es/playing-with-infinit-docker-651236e68cf


////==================================================////
////                         ////
////        Оглавление       ////
////                         ////
////=========================////


  # А. Общая теоретическая часть #
  # ---------------------------- #

    А1. Задачи руководства, технологии и терминология
    А2. Соответствующая требованиям структура проекта
    А3. Работа с git subtree


  # Б. Docker Swarm Mode (DSM) #
  # -------------------------- #

    Б1. Обзор DSM
    Б2. Ключевые концепции DSM
    Б3. Туториал по DSM
    Б4. DSM CLI
    Б5. Деплой docker-compose (v3) в Swarm


  # В. Infinit Storage Engine #
  # ------------------------- #

    В1.


  # X. Практика и наработки #
  # ----------------------- #

    X1.



////==================================================////
////                         ////
////        Содержание       ////
////                         ////
////=========================////

А1. Задачи руководства, технологии и терминология

  --------------------------------------
  Подоглавление:

    # Задачи данного руководства
    # Технологии и терминология

      ▪ Docker и Compose
      ▪ Stateless/Stateful приложения
      ▪ Docker Swarm
      ▪ Infinit Storage Engine
      ▪ VPS/VDS/Дроплет

  --------------------------------------

  > Задачи данного руководства
    - Описать структуру проекта, отвечающую заданным требованиям.
    - Дать практические инструкции для реализаии такого проекта.

  > Технологии и терминология

    • Docker и Compose

      ▪ Что такое Docker
        - Docker позволяет запускать почти любое приложение в контейнере.
        - При этом приложение надёжно изолировано от других контейнеров.
        - Изоляция и безопасность позволяют запускать N контейнеров на 1 хосте.
        - Контейнеры же очень легковесны, ведь они не отягощены гипервизорами.
        - Легковесность контейнеров позволяют получать больше от серверного железа.

      ▪ Docker engine
        - Клиент-серверное приложение.
        - Состоит из Docker-сервера (демона);
          REST API интерфейса для взаимодействия с ним;
          и Docker-клиента (CLI), который взаимодействует с сервером
          через REST API-слой.

      ▪ Docker Hub
        - Docker Hub, это публичный репозиторий от создателей Docker.
        - В нём хранятся образы, которые можно скачивать и использовать.
        - Публичные репозитории бесплатны, приватные платны. Как на github.
        - Образ на Docker Hub связан с соотв.репозиторием на github,
          и при любых изменениях в последнем, срабатывает веб хук,
          и образ на Docker Hub автоматически пересобирается.

      ▪ Docker Compose
        - Это отдельное консольное приложение.
        - Для настройки и запуска много многоконтейнерных приложений.
        - Всё необходимое для этого описывается в 1-ом compose-файле.
        - Compose-файл имеет формат yml.

    • Stateless/Stateful приложения

      ▪ Stateless приложения
        - Это приложения, не имеющие состояния.
        - После выключения никаких данных не сохраняется.
        - Перезагрузка сбрасывает состояние такого приложения к исходному.
        - Например, такое приложение может использовать в качестве хранилища
          данных Redis, но у которого отключено постоянное хранение данных.
        - А вот, например, MySQL такое приложение использовать не может,
          ведь для него надо постоянно хранить данные, которые должны
          сохраняться даже после выключения и перезагрузки приложения.

      ▪ Stateful приложения
        - Это приложения, имеющие и сохраняющие состояние.
        - Его можно включать/выключать/перезагружать, и при очередном
          включении приложение будет находиться в том же состоянии,
          что перед предыдущим выключением.
        - Чтобы приложение было stateful, у него должна быть возможность
          где-то хранить данные о своём состоянии на постоянной основе.
        - Например, используя MySQL. Состояние приложения будет храниться
          в базах данных, которые никуда не исчезают даже после выключения
          приложения.

    • Docker Swarm

      ▪ Общая информация
        - Система объединения экземпляров docker engine в кластер.
        - Сами docker engine могут находиться на разных машинах.
        - Система позволяет осуществлять мониторинг и управление кластером.

      ▪ Менеджер и воркеры
        - Каждому узлу кластера может быть назначена одна из двух ролей.
        - Менеджеры могут управлять кластером, и хранять информацию о нём.
        - Воркеры служат для запуска на них какого-то кода, и работают на кластер.

      ▪ Подробнее
        - Читай в разделе "Б. Docker Swarm".

    • Infinit Storage Engine

      ▪ Бесплатен и open-source
        - На данный момент (03.05.2017) Infinit является open-source проектом.
        - И пользоваться и можно невозбранно и бесплатно.

      ▪ Децентрализованное хранилище
        - Infinit, это софт для организации децентрализованного хранилища данных.
        - Из коробки в community edition поддерживает 3 интерфейса:

          ▪ [block] NBD (Network Block Device)
          ▪ [object] Amazon S3
          ▪ [file] FUSE & Dokan

      ▪ Интеграция с Docker
        - На данный момент (03.05.2017) есть Docker Volume Plugin.
        - Он позволяет docker'у ведить infinit volume, убедиться можно,
          введя команду: docker volume ls.

      ▪ Подробнее
        - Читай в разделе "В. Infinit Storage Engine".

    • VPS/VDS/Дроплет
      - Все термины обозначают в данном руководстве одно и тоже.
      - Виртуальный выделенный сервер, запущенный на базе физического.
      - Некоторые сервисы придумывают свои названия, как, например,
        Digital Ocean называет их "дроплеты".


А2. Соответствующая требованиям структура проекта

  --------------------------------------
  Подоглавление:

    # Требования к структуре проекта
    # Список главных действующих лиц
    # Отделяем stateless-части приложения от stateful-частей
    # Описываем старую структуру, чтобы понять, как превратить её в новую
    # Описываем, как должна выглядеть новая структура проекта

      ▪ Структура файлов и папок
      ▪ Локальная разработка
      ▪ Запуск копий проекта на воркерах

    # Проброс stateless/stateful данных в контейнеры, при разработке / на продакшн

  --------------------------------------

  > Требования к структуре проекта

    • Полное отделение stateful- от stateless-части приложения.
    • Полная интеграция stateless-частей приложения в docker-образы.
    • Хранение stateful-частей всех приложений в одном месте.
    • Мониторинг и управление всеми экземплярами приложения из единого центра.
    • Центр, экземпляры, каждый из них должен располагаться на отдельном vps.
    • Возможность комфортно вести отладку и разработку локально.
    • Docker-образы становятся персонализированными, связанными с конкретным проектом.

  > Список главных действующих лиц
    - CLI-приложение
    - Конфиги
    - Данные MySQL/Redis
    - Логи
    - Всякое прочее
    - Nodejs-приложение
    - Laravel-приложение
    - Папка storage laravel-приложения
    - Docker-образы: server, redis, mysql, nodejs.
    - Github-репозитории: docker-образов, пакетов проекта, самого проекта.

  > Отделяем stateless-части приложения от stateful-частей

    • Stateless
      - Laravel-приложение
      - Nodejs-приложение
      - Docker-образы: server, redis, mysql, nodejs.
      - Github-репозитории: docker-образов, пакетов проекта, самого проекта.
      - CLI-приложение

    • Stateful
      - Данные MySQL/Redis
      - Логи
      - Папка storage laravel-приложения
      - Конфиги
      - Всякое прочее

  > Описываем старую структуру, чтобы понять, как превратить её в новую

    /Github-репозитории: docker-образов
    /Github-репозитории: самого проекта
      .git
      /Конфиги
      /Данные MySQL/Redis
      /Логи
      /CLI-приложение
      /Всякое прочее
      /Nodejs-приложение
      /Laravel-приложение
        /Папка storage laravel-приложения
        /Github-репозитории: пакетов проекта

  > Описываем, как должна выглядеть новая структура проекта

    • Структура файлов и папок

      /app                          | Корневой каталог проекта
        /.git                       | - git-файлы проекта
        /stateless                  | - stateless-часть проекта, компоненты добавляются в прокет, как поддеревья
          /nodejs                   | -- docker-образ, stateless-код
          /mysql                    | -- docker-образ, stateless-код
          /redis                    | -- docker-образ, stateless-код
          /nginx                    | -- docker-образ, stateless-код
            /data                   | --- код, файлы для интеграции в образ с помощью COPY (для этого образа aka project без stateful-составляющей)
              /vendor/4gekkman/[p]  | ---- пакеты вендора 4gekkman, добавляются в проект, как поддеревья
                ...data...          | ----- данные пакета
            DockerFile              | --- описание образа
            readme.md               | --- readme
            .dockerignore           | --- какие файлы не должны попасть на docker-hub
        /stateful                   | - stateful-часть проекта (экземпляры)
          /dev                      | -- экземпляр для локальный разработки
          /instance1                | -- экземпляр №1
          /instance2                | -- экземпляр №2
          /instance3                | -- экземпляр №3
          /instance4                | -- экземпляр №4
            /mysql                  | --- данные mysql
            /redis                  | --- данные redis
            /logs                   | --- логи
            /configs                | --- скомпилированные конфиги
            /storage                | --- папка storage из laravel
            /other                  | --- прочие stateful-данные
        /cli                        | - cli для управления экземплярами на локалке и воркерах
        /configs                    | - группы исходников конфигов
          /dev                      | -- применяются для локальной разработки
          /prod                     | -- применяются для деплоня на воркеты в кластер
        /other                      | - всякий хлам, который может относиться к приложению
        app                         | - точка входа для cli приложения
        autosave2github.ps1         | - powershell-скрипт для push-а во все внешние репы (главный, stateless-образы, пакеты вендора 4gekkman)
        .gitignore                  | - какие файлы не должны попасть в удалённый git-репозиторий;

    • Что содержит .gitignore (stateful-часть приложения)

      /stateful/*

    • Локальная разработка
      - Проект запускается на локальной машине.
      - При этом применяется фаервол.
      - Используются исходники конфигов из папки /dev.
      - CLI-скрипт UP компилирует docker-compose.yml,
        а также прочие конфиги, запускает приложение
        с помощью docker-compose, и настраивает фаервол.
      - Используются stateful-данные из локальной папки local,
        а не пробрасываются из Infinit-volume.
      - Папки со stateless-кодом в контейнерах подменяются
        с помощью volume оными из соотв.папок в stateless.
        Так, что можно разрабатывать код в режиме realtime.

    • Работа на менеджере
      - Для провижн на менеджере используется sh-скрипт,
        но без фаервола.
      - Проект просто-напросто клонируется сюда с github,
        и можно работать.

    • Запуск копий проекта на воркерах
      - Для провижн воркеров используется sh-скрипт,
        но без фаервола.
      - Используются исходники конфигов из папки /prod.
      - CLI-скрипт UP компилирует docker-compose.yml,
        а также прочие конфиги, запускает приложение на воркере
        с помощью docker deploy.
      - Никакого фаервола не применяется, безопасность обеспечивается
        тем, что из контейнеров на хост пробрасываются только те
        порты, которые должны быть общедоступны. А с базами работа
        идёт с локалки, подключив их с менеджера через Infinit.
      - Используются stateful-данные из папки <id воркера>,
        они пробрасываются из Infinit-volume.
      - Самое интересное, что на воркерах вообще не будет
        папки с проектом. Там будут запущены контейнеры,
        как описано в docker-compose, а stateless-часть
        приложения будет интегрирована в контейнеры.
        Но к воркеру будет с помощью infinit подключён volume
        со stateful-данными приложения, которые будут проброшены
        часть в один контейнер, часть в другой.

  > Проброс stateless/stateful данных в контейнеры, при разработке / на продакшн

    • Для локальной версии, конфиги "dev" (приблизительный расклад)

      ▪ Stateful-данные dev с помощью volume

        ▪ Во все сервисы
          - <path_to_the_project>/stateful/<instance_name>/configs:/theproject/data/configs
          - <path_to_the_project>/stateful/<instance_name>/mysql:/theproject/data/mysql
          - <path_to_the_project>/stateful/<instance_name>/redis:/theproject/data/redis
          - <path_to_the_project>/stateful/<instance_name>/logs:/theproject/data/logs
          - <path_to_the_project>/stateful/<instance_name>/other:/theproject/data/other

        ▪ Только в сервис app
          - <path_to_the_project>/stateless/dev/storage:/csgocluster/service/storage

      ▪ Stateless-данные с помощью COPY в DockerFile

        ▪ Во все сервисы
          - COPY ["data", "/csgocluster/service/"]

      ▪ Stateless-данные специально для локальной dev-версиии в цельях разработки

        ▪ Во все сервисы
          - <path_to_the_project>/stateless/projects-csgocluster-app/data:/csgocluster/service
          - <path_to_the_project>/stateless/projects-csgocluster-websockets/data:/csgocluster/service
          - <path_to_the_project>/stateless/projects-csgocluster-mysql/data:/csgocluster/service
          - <path_to_the_project>/stateless/projects-csgocluster-redis/data:/csgocluster/service


А3. Работа с git subtree

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Введение
    # Общие сведения о git subtree

      ▪ Это CLI-команда
      ▪ Её создал Avery Pennarun
      ▪ Позволяет добавлять в подкаталоги подпроекты
      ▪ Не подмодули

    # Справочник по subtree cli

      ▪ git subtree           | показать справку
      ▪ git subtree add       | создать поддерево (импорта из репо), сделать коммит в проект
      ▪ git subtree pull      | делает pull из поддерева из удалённого репозитория
      ▪ git subtree push      | делает push в поддерево удалённого репозитория

      ▪ git subtree split     | извлекает историю поддерева по его <prefix>
      ▪ git subtree merge     |

    # Туториал и практическое руководство

  --------------------------------------

  > Ссылки

    # Официальная документация git subtree
        https://github.com/git/git/tree/master/contrib/subtree

    # The power of Git subtree
        https://developer.atlassian.com/blog/2015/05/the-power-of-git-subtree/

    # Git: submodules vs. subtrees
        https://andrey.nering.com.br/2016/git-submodules-vs-subtrees/

    # git subtrees: a tutorial
        https://medium.com/@v/git-subtrees-a-tutorial-6ff568381844

  > Введение
    - Часто нужно один git-репозиторий включить в другой.
    - Раньше я использовал подмодули, но они меня разочаровали.
    - С ними очень много проблем, поэтому я решил перейти на subtree.
    - В этой главе будет описана практика работы с subtree.

  > Общие сведения о git subtree

    • Это CLI-команда
      - Она дополняет git cli.

    • Её создал Avery Pennarun
      - apenwarr@gmail.com

    • Позволяет добавлять в подкаталоги подпроекты
      - Включая или не включая их полную историю последних.
      - Например, есть проект, в который нужно добавить библиотеку.
      - Вот её как раз и можно добавить в подкаталог проекта, как subtree.

    • Не подмодули
      - Не надо подать subtree с submodule.
      - Хоть у них и одинаковые задачи, но решения это разные.
      - В отличие от подмодулей, поддеревья:

        ▪ Не требуют наличия в репозитории конструкций типа .gitmodule или gitlinks.
        ▪ Конечному пользователю не нужно что-то знать о поддеревьях.
        ▪ Содержимое каталогов-поддеревьев содержится на github-репозитории проекта,
          и при клонировании проекта с github также клонируется.

  > Справочник по subtree cli

    •••••••••••••••••••••
    •• git subtree add ••
    •••••••••••••••••••••

      ▪ Параметры

        --prefix=<prefix> <commit>
        --prefix=<prefix> <repository> <ref>

      ▪ Описание
        - Создать поддерево <prefix>.
        - Импортировав содержимое из <repository>, ветки <ref>.
        - Новый коммит в основной проект будет создан автоматом.

      ▪ Опции

        --squash  | импортировать только 1 коммит из подпроекта, а не всю историю

      ▪ Примеры
        - Допустим, надо добавить в проект app подпроект git-subtree.
        - Делаем cd в каталог app.
        - И выполняем:

          git subtree add --prefix=git-subtree --squash git://github.com/apenwarr/git-subtree.git master

    ••••••••••••••••••••••
    •• git subtree pull ••
    ••••••••••••••••••••••

      ▪ Параметры

        --prefix=<prefix> <repository> <ref>

      ▪ Описание
        - Извлекает свежие данные в поддерево <prefix>.
        - Из ветки <ref> удалённого <repository>

      ▪ Опции

        --squash  | импортировать только 1 коммит из подпроекта, а не всю историю

      ▪ Примеры

        git subtree pull --prefix=gitweb git@github.com:whatever/gitweb.git master


    ••••••••••••••••••••••
    •• git subtree push ••
    ••••••••••••••••••••••

      ▪ Параметры

        --prefix=<prefix> <repository> <ref>

      ▪ Описание
        - Выполняет split, используя <prefix> (извлекает его историю).
        - Делает git push в ветку <ref> удалённого <repository>.

      ▪ Опции

        --squash  | импортировать только 1 коммит из подпроекта, а не всю историю

      ▪ Примеры

        git subtree push git@github.com:whatever/gitweb.git gitweb-latest:master

    •••••••••••••••••••••••
    •• git subtree split ••
    •••••••••••••••••••••••

      - TODO: Описать по запросу.

    •••••••••••••••••••••••
    •• git subtree merge ••
    •••••••••••••••••••••••

      - TODO: Описать по запросу.

  > Туториал и практическое руководство

    1. Создадим локальный и удалённый репозитории parent
      - Будем использовать его, как основной проект.
      - Создай папку parent, в ней readme.md и file2,
        добавь в них какой-нибудь текст.
      - Сделай git init в этой папке.
      - Создай удалённый репозиторий parent на github.
      - Сделай свяжи локальный репозиторий с удалённым:

          git remote add parent git@github.com:4gekkman/parent.git

      - Выполни первый push в него (из linux, у которого есть доступ к репозиторию):

          git add .
          git add -u .
          git commit -m test
          git push parent master:master

    2. Создадим локальный и удалённый репозитории my-subproject
      - Локально он должен находиться вне папки с parent.
      - Его мы будем подключать в parent, как subtree.
      - Создай папку my-subproject, в ней readme.md и file1,
        добавь в них какой-нибудь текст.
      - Сделай git init в этой папке.
      - Создай удалённый репозиторий my-subproject на github.
      - Сделай свяжи локальный репозиторий с удалённым:

          git remote add my-subproject git@github.com:4gekkman/my-subproject.git

      - Выполни первый push в него (из linux, у которого есть доступ к репозиторию):

          git add .
          git add -u .
          git commit -m test
          git push my-subproject master:master

    3. Добавим my-subproject, как поддерево в parent
      - Делаем cd в parent.
      - И добавляем my-subproject в parent, как поддерево,
        в папку my-subproject/ (в неё будет скопировано содержимое,
        не забудь / в конце; ниже ссылка для анонимного скачивания без ключа):

          git subtree add --prefix=my-subproject/ git://github.com/4gekkman/my-subproject.git master    // Для публичного репозитория
          git subtree add --prefix=my-subproject/ git@github.com:4gekkman/my-subproject.git master      // Для приватного репозитория с доступом через ключ

      - Коммит уже автоматически сделался в parent.
      - Так что, теперь просто делаем push:

          git push parent master:master

      - Заходим на github, и убеждаемся, что там появилась папка my-subproject:
        https://github.com/4gekkman/parent
      - При всём при этом, папка my-subproject не содержит папки .git.
        То есть, она, по сути, часть .git всего проекта.

    4. Меняем локально my-subproject и parent, но делаем push только в parent
      - Мы можем как угодно менять my-subproject и parent.
      - И делать commit и push в parent прямо из my-subproject.
      - Но при этом my-subproject на github никак меняться не будет,
        а все изменения будут поступать только в parent.
      - Для примера:

        ▪ В parent изменим текст в file2.
        ▪ В my-subproject добавим file3.

      - Теперь сделаем commit и выполним push в parent:

        git add .
        git add -u .
        git commit -m test
        git push parent master:master

      - После этого открываем github-репозитории parent и
        my-subproject. В первом изменения видны, а вот
        последний не изменился.
        https://github.com/4gekkman/parent/tree/master/my-subproject
        https://github.com/4gekkman/my-subproject

    5. Внесённые локально изменения push'им в my-subproject
      - В п.4 мы в т.ч. внесли локальные изменения в поддерево my-subproject.
      - Теперь мы хотим сделать push этих изменений в my-subproject на github.
      - Для этого:

        git add .
        git add -u .
        git commit -m test
        git subtree push --prefix=my-subproject git@github.com:4gekkman/my-subproject.git master

      - После этого открываем github-репозиторий my-subproject.
        И видим, что изменения были внесены.


Б1. Обзор DSM

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Введение

      ▪ О чём эта глава
      ▪ Docker Engine (DE) в режиме Swarm
      ▪ Что позволяет DE в режиме Swarm

    # Обзор функционала Docker Swarm Mode (DSM)

      ▪ Управление кластором в Docker Engine
      ▪ Децентрализованный дизайн
      ▪ Декларативная модель сервисов
      ▪ Масштабируемость
      ▪ Автоматическая поддержка заданного состояния
      ▪ Общая сеть между узлами роя на разных хостах
      ▪ Обнаружение сервисов
      ▪ Балансировка нагрузки
      ▪ Безопасен по умолчанию
      ▪ Инкрементальное обновление

  --------------------------------------

  > Ссылки

    # Обзор Docker Swarm, официальная документация
        https://docs.docker.com/engine/swarm/

  > Введение

    • О чём эта глава
      - В этой главе общий обзор Docker Swarm.

    • Docker Engine (DE) в режиме Swarm
      - До версии DE 1.12.0, Docker Swarm был отделён от него.
      - С версии DE 1.12.0, Docker Swarm является частью DE.
      - Как говорят, "DE может работать в режиме Swarm".

    • Что позволяет DE в режиме Swarm
      - Управлять кластером Docker Engine'ов.
      - Такую структуру называют Swarm'ом, т.е. Роем. Рой Docker Engine'ов.
      - Можно деплоить приложения в рой, и управлять им.

  > Обзор функционала Docker Swarm (DSM)

    • Управление кластором в Docker Engine
      - Управление кластером интегрировано в Docker Engine.
      - Рой можно создать с помощью DE CLI.
      - Доп. софт для создания и управления роем не нужен.

    • Децентрализованный дизайн
      - В DSM есть 2 типа узлов, менеджеры и воркеры.
      - Но нет необходимости назначать роли в момент деплоя.
      - Деплоить можно на любой узел, независимо от роли.
      - Это, например, позволяет построить рой из одного образа.

    • Декларативная модель сервисов
      - DE использует декларативный подход к определению сервисов приложения.
      - Например, оно может состоять из нескольких сервисов: app, mysql, redis, websockets.

    • Масштабируемость
      - Для каждого сервиса можно указать, какое число задач д.б. запущено.
      - При изменении числа задач, DSM автоматически создаёт новые/прибивает лишние.

    • Автоматическая поддержка заданного состояния
      - Узел-менеджер постоянно мониторит состояние роя.
      - Если что-либо упало, он сам это поднимает автоматически.

    • Общая сеть между узлами роя на разных хостах
      - Можно поднять оную для обеспечения надёжной связи между узлами.
      - Например, это как раз понадобится при работе с Docker Infinit.
      - Stateful-данные будут смонтированы с менеджера на воркер через такую сеть.

    • Обнаружение сервисов
      - Узел-менеджер назначает каждому узлу роя уникальное DNS-имя.
      - А в DE есть встроенный DNS-сервер.
      - Так что, в любое время можно обратиться по имени к любому узлу роя.

    • Балансировка нагрузки
      - Во-первых, можно пробросить порты из контейнеров на хост,
        что сделает соответствующие сервисы доступными из интернета.
      - Во-вторых, можно конкретно указывать DSM, какие контейнеры
        на каких узлах надо запускать.

    • Безопасен по умолчанию
      - Применяется TLS аутентификация и шифрование.
      - Поэтому узлы общаются между собой достаточно безопасным образом.
      - Можно даже использовать сертификаты.

    • Инкрементальное обновление
      - Применять обновления можно к узлам по очереди.
      - Если что-то пойдёт не так, можно откатиться к предыдущей версии.


Б2. Ключевые концепции DSM

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Введение

    # Что есть рой?

      ▪ SwarmKit
      ▪ Swarm Mode
      ▪ Что есть рой?
      ▪ Разница между Swarm и не-Swarm режимом

    # Что есть узел роя?

      ▪ Экземпляр DE, участвующие в рое
      ▪ Как задеплоить сервис в рой?
      ▪ Выборы лидера среди менеджеров
      ▪ Узлы-воркеры и агенты на них

    # Что есть сервисы и задачи?

      ▪ Что есть сервис?
      ▪ Режимы работы сервисов: replicated и global
      ▪ Что есть задача?

    # Балансировка нагрузки

  --------------------------------------

  > Ссылки

    # Ключевые концепции Docker Swarm Mode
        https://docs.docker.com/engine/swarm/key-concepts/

  > Введение
    - В этой главе описаны ключевые концепции DSM.

  > Что есть рой?

    • SwarmKit
      - Система управления кластерами в DE постноена на основе SwarmKit.

    • Swarm Mode
      - Участвующие в рое DE'ы работают в режиме роя (Swarm Mode).
      - Включить режим роя можно двумя способами:

        1. Подключить DE к существующему рою.
        2. Создать новый рой, первым узлом которого и будет этот DE.

    • Что есть рой?
      - Кластер узлов, где на каждом Docker Engine в Swarm Mode.
      - DE CLI и API включают команды для управления роем.
      - Можно добавлять/удалять узлы, деплоить и управлять сервисами в рое.

    • Разница между Swarm и не-Swarm режимом
      - Отдавая команды DE в не-Swarm режиме, вы выполняете команды контейнера.
      - А в Swarm-режиме, вы управляете сервисами.
      - Причём, можно на 1-ом DE запустить и swarm-сервис, и отдельный контейнер.

  > Что есть узел роя?

    • Экземпляр DE, участвующие в рое
      - Экземпляр DE, участвующие в рое (т.е. находящийся в Swarm Mode).
      - Обычно в продакшн на одном сервере работает лишь один узел.
      - Хотя, для извращенцев, есть способ сделать много узлов на 1 сервере.

    • Как задеплоить сервис в рой?
      - Надо предоставить определение сервиса узлу-менеджеру.
      - А он уже отправит задание воркерам организовать соотв.сервис.

    • Выборы лидера среди менеджеров
      - Узлы-менеджеры также выполняют функции управления роем.
      - Они следят и поддерживают желаемое состояние роя.
      - Узлы-менеджеры выбирают между собой единого лидера, который и занимается.

    • Узлы-воркеры и агенты на них
      - Получают и выполняют задачи от узлов-менеджеров.
      - Хотя, узлы-менеджеры могут и работать воркерами тоже.
      - Но можно отключить возможность узла-менеджера работать воркером.
      - На каждом узле-воркере работает агент, который докладывает менеджеру.

  > Что есть сервисы и задачи?

    • Что есть сервис?
      - Тоже самое, что сервис в docker-compose.yml.
      - Указываешь 1 docker-образ, всякие параметры, и т.п.

    • Режимы работы сервисов: replicated и global

      ▪ replicated
        - Узел-менеджер запускает указанное кол-во копий сервиса в рое.
        - Если не указать ограничений, он сам выберет, на каких узлах.

      ▪ gloval
        - Узел-менеджер запускает по 1 задаче на каждом узле роя.

    • Что есть задача?
      - Docker-контейнер и команды для запуска в нём.
      - Задача является строительным кирпичиком в планировщике роя.
      - Узел-менеджер шлёт воркерам столько задач, сколько надо создать реплик.
      - Будучи назначенной узлу, задача не может улететь на другой.
      - Она либо будет выполнена на этом узле, либо помрёт.

  > Балансировка нагрузки
    - DSM использует т.н. "ingress load balancing".
    - Т.Е., выставляет наружу только порты сервисов, которые д.б. доступны.
    - Эти порты можно либо самому назначать, либо получать автоматом из диапазона 30000-32767.
    - Принцип работы такой же, как в docker-compose.


Б3. Туториал по DSM

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Введение

    # Создаём рой

      1. Подключаемся к менеджеру через SSH
      2. Инициируем создание роя
      3. Проверяем текущее состояние роя
      4. Получаем список узлов в кластере

    # Добавляем узлы в рой

      1. Подключаемся к воркеру через SSH
      2. Подключаем воркера к рою
      3. Проверяем список узлов в кластере

    # Деплоим сервис (контейнер) в рой

      1. Подключаемся к менеджеру через SSH
      2. Деплоим сервис (для примера, просто контейнек alpine)
      3. Проверяем список запущенных сервисов
      4. Узнаём, на каких узлах запущен сервис
      5. Получаем подробную инфу о запущенном сервисе
      6. Узнаём подробности о контейнере сервиса

    # Удаляем сервис

      1. Подключаемся к менеджеру через SSH
      2. Удаляем сервис
      3. Проверяем, что сервис удалился, с менеджера
      4. Проверяем, что контейнеры удалились, на узлах, где работал сервис

    # Обновляем сервис

      1. Подключаемся к менеджеру через SSH
      2. Деплоим redis 3.0.6, чтобы потом обновить до 3.0.7
      3. Получаем подробную инфу о запущенном сервисе
      4. Обновляем образ redis'а сервиса на новую версию
      5. Получаем подробную инфу о запущенном сервисе (проверяем, обносился ли)
      6. Наблюдать за состоянием накатывающихся обнвлений

  --------------------------------------

  > Ссылки

    # Официальный туториал по DSM
        https://docs.docker.com/engine/swarm/swarm-tutorial/

  > Введение

    • Что за туториал
      - Небольшой туториал, покрывающий базовые действия с кластером.
      - Инициализация кластера, добавление узлов, деплой и управление.
      - Все дейтвия будут проводиться в консоли с помощью DE CLI.

    • Что понадобится
      - Два компа/виртуалки с Linux и установленным Docker Engine.
      - Один из них с реальным IP, и у них должен быть интернет.
      - Между ними должны быть открыты следующие порты:

        ▪ TCP 2377              | для управления кластером
        ▪ TCP и UDP 7946        | для общения между узлами
        ▪ UDP 4789              | для трафика через overlay-сеть между узлами
        ▪ ip protocol 50 (ESP)  | (--opt encrypted) для шифрованного трафика через overlay-сеть

  > Создаём рой

    1. Подключаемся к менеджеру через SSH
    2. Инициируем создание роя

      • Инициируем

        docker swarm init --advertise-addr <реальный IP менеджера>

      • В ответ получаем что-то типа этого:

        Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.
        To add a worker to this swarm, run the following command:

            docker swarm join \
            --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
            192.168.99.100:2377

        To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

      • Пояснения
        - В ответе есть токен, он нужен будет для подключения к рою узлов.
        - Там даже есть команда, с помощью которой можно подключать новые узлы к рою.

    3. Проверяем текущее состояние роя

      • Проверяем

        docker info

      • В ответ получаем что-то типа этого:

        Containers: 2
        Running: 0
        Paused: 0
        Stopped: 2
          ...snip...
        Swarm: active
          NodeID: dxn1zf6l61qsb1josjja83ngz
          Is Manager: true
          Managers: 1
          Nodes: 1
          ...snip...

    4. Получаем список узлов в кластере

      • Получаем

        docker node ls

      • В ответ получаем что-то типа этого:

        ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
        dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader

      • Что означает звёздочка
        - Она показывает узел, к которому тыподключен через SSH.

  > Добавляем узлы в рой

    1. Подключаемся к воркеру через SSH
    2. Подключаем воркера к рою

      • Как посмотреть команду для подключения воркера к рою
        - Надо подключиться к менеджеру через SSH.
        - И ввести команду (даст тот же вывод, что при инициализации роя):

          docker swarm join-token worker

      • Подключаем воркера

          docker swarm join --token <токен> <ip менеджера>:2377

    3. Проверяем список узлов в кластере

      • Проверяем
        - Подключаемся к менеджеру через SSH.
        - Вводим:

          docker node ls

      • В ответ получаем что-то типа этого:

          ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
          9j68exjopxe7wfl6yuxml7a7j    worker1   Ready   Active
          dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader

  > Деплоим сервис (контейнер) в рой

    1. Подключаемся к менеджеру через SSH
    2. Деплоим сервис (для примера, просто контейнек alpine)

      • Деплоим

        docker service create --replicas 1 --name helloworld alpine ping docker.com

      • В ответ получаем ID созданного сервиса

        9uk4639qpg7npwf3fn2aasksr

    3. Проверяем список запущенных сервисов

      • Проверяем

        docker service ls

      • В ответ получаем что-то типа

        ID            NAME        SCALE  IMAGE   COMMAND
        9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com

    4. Узнаём, на каких узлах запущен сервис

      • Узнаём

        docker service ps <ID сервиса>

      • В ответ получаем что-то типа

        NAME                                    IMAGE   NODE     DESIRED STATE  LAST STATE
        helloworld.1.8p1vev3fq5zm0mi8g0as41w35  alpine  worker2  Running        Running 3 minutes

    5. Получаем подробную инфу о запущенном сервисе

      • Получаем

        docker service inspect --pretty <ID сервиса>

      • Ответ
        - В ответ придёт JSON с подробной инфой.
        - Это, по сути, тоже самое, что inspect для контейнера.


    6. Узнаём подробности о контейнере сервиса
      - Подключаемся через SSH к узлу, где работает сервис.
      - Выполняем там:

        docker ps

  > Удаляем сервис

    1. Подключаемся к менеджеру через SSH
    2. Удаляем сервис
      - В нашем туториале, сервис назывался helloworld.
      - Его и удаляем:

        docker service rm helloworld to remove the helloworld

    3. Проверяем, что сервис удалился, с менеджера
      - С помощью docker inspect:

        docker service inspect helloworld

    4. Проверяем, что контейнеры удалились, на узлах, где работал сервис
      - Надо к каждому такому узлу подключиться через SSH.
      - И убедиться, что контейнеры удалены:

        docker ps

  > Обновляем сервис

    1. Подключаемся к менеджеру через SSH
    2. Деплоим redis 3.0.6, чтобы потом обновить до 3.0.7

      • Деплоим

        docker service create \
          --replicas 3 \
          --name redis \
          --update-delay 10s \
          redis:3.0.6

      • Как проходят обновления
        - По умолчанию, планировщик обновляет 1 задачу за раз.
          Т.Е. --update-parallelism == 1. Но можно и изменить его.
        - С помощью --update-delay можно выставить задержку между обновлениями.
        - После очередного обновления, ситуация зависит от результата:

          1) Если успех, и сервис RUNNING
            - Планировщик начинает обновлять следующую задачу.

          2) Если неудача, и сервис FAILED
            - Планировщик ставит обновленяи на паузу.

        - Настраивать вышеописанное поведение можно с помощью флага
          --update-failure-action для docker service create или docker service update.

    3. Получаем подробную инфу о запущенном сервисе

        docker service inspect --pretty redis

    4. Обновляем образ redis'а сервиса на новую версию

      • Обновляем

        docker service update --image redis:3.0.7 redis

      • Планировщик накатывает обновление следующим образом (по умолчанию)

        ▪ Останавливает 1-ю задачу.
        ▪ Добавляет задачу по обновлению в планировщик для остановленной задачи.
        ▪ Запускает контейнер обновлённой задачи.
        ▪ Если обновлённая задаче вернула RUNNING, ждёт указанное в --update-delay время,
          и начинает обновлять следующую задачу.
        ▪ Если, в любой момент во время обновления, задача вернула FAILED,
          весь процесс обновления ставится на паузу.

    5. Получаем подробную инфу о запущенном сервисе (проверяем, обносился ли)

        docker service inspect --pretty redis

    6. Наблюдать за состоянием накатывающихся обнвлений

        docker service ps <ID сервиса>


Б4. DSM CLI

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Введение

    # Справочник по командам docker swarm

      ▪ init          | Инициировать создание роя
      ▪ join          | Подключить узел к рою, как воркер/менеджер
      ▪ join-token    | Посмотреть токен для подключения к рою
      ▪ leave         | Отключить узел от роя
      ▪ unlock        | Разблокировать рой
      ▪ unlock-key    | Посмотреть ключ для разблокировки роя
      ▪ update        | Обновить рой

    # Справочник по командам docker stack

      ▪ deploy        | Деплой нового стека, или обновление существующего
      ▪ ls            | Посмотреть список работающих стеков
      ▪ ps            | Посмотреть работающие задачи указанного стека
      ▪ rm            | Удалить указанный стэк
      ▪ services      | Посмотреть работающие сервисы указанного стека

  --------------------------------------

  > Ссылки

    # docker swarm
        https://docs.docker.com/engine/reference/commandline/swarm/

    # docker stack
        https://docs.docker.com/engine/reference/commandline/stack/

    # Официальный справочник по Composer-file v3
        https://docs.docker.com/compose/compose-file/

  > Введение
    - В этом CLI-справочнике будут рассмотрены docker swarm/stack.
    - Swarm понятно почему, а stack позволяет деплоить в рой
      с помощью файла docker-compose.yml (об этом следующая глава).

  > Справочник по командам docker swarm

    • docker swarm init [OPTIONS]

      ▪ Описание
        - Инициировать создание роя.

      ▪ Опции

        --advertise-addr          | IP-адрес менеджера
        --autolock                | [false] Повесить на менеджер замок (нужен ключ для остановки/запуска)
        --cert-expiry             | [2160h0m0s] Период валидности для сертификатов узлов
        --dispatcher-heartbeat    | [5s] Время сердцебиения диспетчера
        --external-ca             | Ссылки на 1 или более внешних центров подписи сертификатов
        --force-new-cluster       | [false] Заставить создать новый кластер из текущего состояния
        --listen-addr             | [0.0.0.0:2377] Какой адрес слушать
        --max-snapshots           | [0] Кол-во доп.снэпшотов для сохранения (0 по умолчанию)
        --snapshot-interval       | [10000] Кол-во записей в лог перед тем, как будет снят снэпшот
        --task-history-limit      | [5] Кол-во задач и истории

      ▪ Пример

        docker swarm init --advertise-addr 192.168.99.121

    • docker swarm join [OPTIONS] HOST:PORT

      ▪ Описание
        - Подключить узел к рою, как воркер/менеджер.

      ▪ Опции

        --token                   | Токен для подключения к рою
        --advertise-addr          | IP-адрес менеджера
        --listen-addr             | [0.0.0.0:2377] Какой адрес слушать

      ▪ Где брать токен

        1. Он выводится при создании роя командой docker swarm init.
        2. Можно зайти на узел роя и ввести:

          docker swarm join-token worker

      ▪ Пример

          docker swarm join --token SWMTKN-1-3pu6hszjas19xyp7ghgosyx9k8atbfcr8p2is99znpy26u2lkl-7p73s1dx5in4tatdymyhg9hu2 192.168.99.121:2377

    • docker swarm join-token [OPTIONS] (worker|manager)

      ▪ Описание
        - Посмотреть токен для подключения к рою.
        - Выполнять её надо на узле-менеджере.
        - Указывать "worker", если нужен токен для подключения воркера.
        - Указывать "manager", если нужен токен для подключения менеджера.

      ▪ Опции

        --quiet, -q               | [false] Показать только токен, без лишней инфы
        --rotate                  | [false] Обновить токен

    • docker swarm leave [OPTIONS]

      ▪ Описание
        - Отключить узел от роя.

      ▪ Опции

        --force, -f               | [false] Принудить узел покинуть рой, игнорировать предупреждения

      ▪ Пример

          docker swarm leave    | Отключить
          docker node ls        | Проверить

    • docker swarm unlock

      ▪ Описание
        - Разблокировать рой.

      ▪ Опции
        - Нет.

    • docker swarm unlock-key [OPTIONS]

      ▪ Описание
        - Посмотреть ключ для разблокировки роя.
        - Выполнять её надо на узле-менеджере.

      ▪ Опции

        --quiet, -q               | [false] Показать только токен, без лишней инфы
        --rotate                  | [false] Обновить токен

    • docker swarm update [OPTIONS]

      ▪ Описание
        - Обновить рой.

      ▪ Опции

        --autolock                | [false] Повесить на менеджер замок (нужен ключ для остановки/запуска)
        --cert-expiry             | [2160h0m0s] Период валидности для сертификатов узлов
        --dispatcher-heartbeat    | [5s] Время сердцебиения диспетчера
        --external-ca             | Ссылки на 1 или более внешних центров подписи сертификатов
        --max-snapshots           | [0] Кол-во доп.снэпшотов для сохранения (0 по умолчанию)
        --snapshot-interval       | [10000] Кол-во записей в лог перед тем, как будет снят снэпшот
        --task-history-limit      | [5] Кол-во задач и истории

      ▪ Пример

          docker swarm update --cert-expiry 720h

  > Справочник по командам docker stack

    • docker stack deploy [OPTIONS] STACK

      ▪ Описание

        ▪ Краткое описание
          - Деплой нового стека, или обновление существующего.

        ▪ Запускать команду нужно на узле-менеджере
          - Вот так вот.

        ▪ Команда deploy поддерживает compose-файлы от v3 и выше
          - Так вот так.

      ▪ Опции

        --bundle-file             | Путь к бандл-файлу (dab) приложения
        --compose-file, -c        | Путь к compose-файлу
        --with-registry-auth      | Отправить аутентиф.детали реестра агентам роя

      ▪ Пример

        ▪ Деплоим
          - Задеплоим сервис под названием vossibility в рой.
          - При этом, docker-compose.yml лежит в текущем каталоге.
          - Для этого выполним:

              docker stack deploy --compose-file docker-compose.yml vossibility

          - В качестве вывода увидим что-то вроде:

              Creating network vossibility_vossibility
              Creating network vossibility_default
              Creating service vossibility_nsqd
              Creating service vossibility_logstash
              Creating service vossibility_elasticsearch
              Creating service vossibility_kibana
              Creating service vossibility_ghollector
              Creating service vossibility_lookupd

        ▪ Удостоверяемся, что всё ок
          - Удостовериться, что сервис успешно запущен, можно так:

              docker service ls

          - Вывод будет примерно такой:

              ID            NAME                               MODE        REPLICAS  IMAGE
              29bv0vnlm903  vossibility_lookupd                replicated  1/1       nsqio/nsq@sha256:eeba05599f31eba418e96e71e0984c3dc96963ceb66924dd37a47bf7ce18a662
              4awt47624qwh  vossibility_nsqd                   replicated  1/1       nsqio/nsq@sha256:eeba05599f31eba418e96e71e0984c3dc96963ceb66924dd37a47bf7ce18a662
              4tjx9biia6fs  vossibility_elasticsearch          replicated  1/1       elasticsearch@sha256:12ac7c6af55d001f71800b83ba91a04f716e58d82e748fa6e5a7359eed2301aa
              7563uuzr9eys  vossibility_kibana                 replicated  1/1       kibana@sha256:6995a2d25709a62694a937b8a529ff36da92ebee74bafd7bf00e6caf6db2eb03
              9gc5m4met4he  vossibility_logstash               replicated  1/1       logstash@sha256:2dc8bddd1bb4a5a34e8ebaf73749f6413c101b2edef6617f2f7713926d2141fe
              axqh55ipl40h  vossibility_vossibility-collector  replicated  1/1       icecrime/vossibility-collector@sha256:f03f2977203ba6253988c18d04061c5ec7aab46bca9dfd89a9a1fa4500989fba

          - Можно заметить, что имя стэка появляется в виде префикса в именах сервисов.
          - Поэтому, легко можно разделять сервисы разных стеков.

    • docker stack ls

      ▪ Описание
        - Посмотреть список работающих стеков.

      ▪ Опции
        - Нет.

      ▪ Пример
        - Вводим: docker stack ls
        - Получаем что-то вроде:

            ID                 SERVICES
            vossibility-stack  6
            myapp              2

        - Как видно, справа показано кол-во работающих в стеке сервисов.
        - Так что, если что-то отвалится, можно спалить по этому числу =)

    • docker stack ps [OPTIONS] STACK

      ▪ Описание
        - Посмотреть работающие задачи (контейнеры) указанного стека.

      ▪ Опции

        --filter, -f      | Фильтровать вывод указанными условиями
        --no-resolve      | [false] Не map'ить ID с именами.
        --no-trunc        | [false] Не обрезать вывод

      ▪ Про флильтрация (вырезка из оригинальной документации)

          The filtering flag (-f or --filter) format is a key=value
          pair. If there is more than one filter, then pass
          multiple flags (e.g. --filter "foo=bar" --filter "bif=baz").
          Multiple filter flags are combined as an OR filter.
          For example, -f name=redis.1 -f name=redis.7 returns both
          redis.1 and redis.7 tasks.

          The currently supported filters are:

           ▪ id
           ▪ name
           ▪ desired-stat

    • docker stack rm STACK

      ▪ Описание
        - Удалить указанный стэк.

      ▪ Опции
        - Нет.

    • docker stack services [OPTIONS] STACK

      ▪ Описание
        - Посмотреть работающие сервисы указанного стека.

      ▪ Опции

        --filter, -f      | Фильтровать вывод указанными условиями
        --no-resolve      | [false] Не map'ить ID с именами.
        --no-trunc        | [false] Не обрезать вывод

      ▪ Про флильтрация (вырезка из оригинальной документации)

          The filtering flag (-f or --filter) format is a key=value
          pair. If there is more than one filter, then pass
          multiple flags (e.g. --filter "foo=bar" --filter "bif=baz").
          Multiple filter flags are combined as an OR filter.
          For example, -f name=redis.1 -f name=redis.7 returns both
          redis.1 and redis.7 tasks.

          The currently supported filters are:

           ▪ id
           ▪ name
           ▪ desired-stat

      ▪ Пример

        ▪ Посмотреть сервисы стека myapp

          docker stack services myapp

        ▪ Вывод примерно такой

          ID            NAME            REPLICAS  IMAGE                                                                          COMMAND
          7be5ei6sqeye  myapp_web       1/1       nginx@sha256:23f809e7fd5952e7d5be065b4d3643fbbceccd349d537b62a123ef2201bc886f
          dn7m7nhhfb9y  myapp_db        1/1       mysql@sha256:a9a5b559f8821fe73d58c3606c812d1c044868d42c63817fa5125fd9d8b7b539


Б5. Деплой docker-compose (v3) в Swarm

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Введение

      ▪ О чём эта глава
      ▪ Начиная с DE версии 1.13

    # Опция deploy в Compose-файле

      ▪ Что за опция
      ▪ Подопции

    # Справочник по опции deploy и её подопциям

      ▪ mode                  | Режим работы сервиса, global / replicated
      ▪ replicas              | Для режима replicated, задаёт кол-во контейнеров сервиса, которые должные одновременно работать.
      ▪ placement             | Определяет ограничения на размещение сервиса
        ▪ constraints         | Конкретные ограничения

      ▪ update_config         | Настройки обновления сервиса, как в docker service create
        ▪ parallelism         | Какое кол-во контейнеров обновлять за 1 итерацию
        ▪ delay               | Задержка между итерациями обновления контейнеров
        ▪ failure_action      | Что делать в случае ошибки, pause(default)/continue
        ▪ monitor             | Сколько ждать после обновления, что вылезет ошибка (0s по умолчанию)
        ▪ max_failure_ratio   | Сколько ошибок стерпеть при обновлении

      ▪ resources             | Настройка ограничений на системные ресурсы для сервиса
        ▪ limits              | Ограничения на ресурсы
          ▪ cpus              | Ограничения на CPU
          ▪ memory            | Ограничения на память
        ▪ reservations        | Резервация ресурсов
          ▪ cpus              | Резервация ресурсов CPU
          ▪ memory            | Резервация ресурсов памяти

      ▪ restart_policy        | Настройка, нужно ли и как перезапускать контейнеры, если они завершают работу
        ▪ condition           | [default] none / on-failure / any
        ▪ delay               | [0] Как долго ждать между попытками рестарта
        ▪ max_attempts        | [never give up] MAX число попыток рестарта
        ▪ window              | [decide immediately] Как долго ждать, прежде чем решить, что рестарт удался

      ▪ labels                | Назначить ярлыки для сервиса

    # Туториал по деплою compose-сервиса в swarm

      1. Подготовим compose-файл
      2. Задеплоим его на менеджера
      3. Проверим, создался ли стэк, и число задач в нём
      4. Проверим состояние контейнеров стека

  --------------------------------------

  > Ссылки

    # Сторонний, но хороший туториал по теме главы
        https://technologyconversations.com/2017/01/23/using-docker-stack-and-compose-yaml-files-to-deploy-swarm-services/

    # Всякие прочие статьи/туториалы, которые я нашёл
        https://docs.docker.com/compose/swarm/#multiple-dependencies
        http://blog.terranillius.com/post/composev3_swarm/
        http://blog.arungupta.me/deploy-docker-compose-services-swarm/

  > Введение

    • О чём эта глава
      - В этой главе туториал
      - По деплою docker-compose v3 приложения в рой.

    • Начиная с DE версии 1.13
      - Появилась поддержка Compose YAML-файлов v3.
      - И соответствующая команда в Docker CLI, docker stack.
      - Команда работает только с 3-й версией compose-файлов.

  > Опция deploy в Compose-файле

    • Что за опция
      - Для конфигурации сервиса для команды docker stack deploy.
      - То есть, при запуске compose-приложения оной командой.
      - Она игнорируется при использовании с docker-compose up.

    • Подопции
      - У опций внутри deploy также есть и подопции.
      - Подробнее обо всех опциях и подопциях deploy см.ниже.
      - Например:

        version: '3'
        services:
          worker:
            image: dockersamples/examplevotingapp_worker
            deploy:
              replicas: 6
              update_config:
                parallelism: 2
                delay: 10s
              restart_policy:
                condition: on-failure

  > Справочник по опции deploy и её подопциям

    • mode

      ▪ Описание
        - Режим работы сервиса.
        - Возможные значения: global / replicated.

      ▪ Режимы работы сервиса

        ▪ global:     | ровно по 1-му контейнеру на каждому узле.
        ▪ replicated  | [по умолчанию] указанное количество контейнеров

      ▪ Пример

          deploy:
            mode: global

    • replicas

      ▪ Описание
        - Для режима replicated, задаёт кол-во контейнеров сервиса, которые должные одновременно работать.

      ▪ Только для mode == replcated
        - Вот так вот.

      ▪ Пример

          deploy:
            mode: replicated
            replicas: 6

    • placement

      ▪ Описание
        - Определяет ограничения на размещение сервиса.
        - Например, можно приказать размещать на определённом узле.

      ▪ Подопции

        ▪ constraints     | Конкретные ограничения

      ▪ Справочник по видам ограничений

        ▪ node.id           | node.id == 2ivku8v2gvtg4
          - ID узла.

        ▪ node.hostname     | node.hostname != node-2
          - Имя хоста узла.

        ▪ node.role         | node.role == manager
          - Роль узла.

        ▪ node.labels       | node.labels.security == high
          - Определённые пользователем ярлыки узла.

        ▪ engine.labels     | engine.labels.operatingsystem == ubuntu 14.04
          - Определённые DE ярлыки узла.

      ▪ Пример

          deploy:
            placement:
              constraints:
                - node.role == manager
                - engine.labels.operatingsystem == ubuntu 14.04

    • update_config

      ▪ Описание
        - Настройки обновления сервиса.
        - Это те же самые опции обновления из docker service create.

      ▪ Подопции

        ▪ parallelism         | Какое кол-во контейнеров обновлять за 1 итерацию
        ▪ delay               | Задержка между итерациями обновления контейнеров
        ▪ failure_action      | Что делать в случае ошибки, pause(default)/continue
        ▪ monitor             | Сколько ждать после обновления, что вылезет ошибка (0s по умолчанию)
        ▪ max_failure_ratio   | Сколько ошибок стерпеть при обновлении

    • resources

      ▪ Описание
        - Настройка ограничений на системные ресурсы для сервиса.
        - Заменяет соотв.опции из старых версий compose-файла:
          cpu_shares, cpu_quota, cpuset, mem_limit, memswap_limit, mem_swappiness.

      ▪ Подопции

        ▪ limits              | Ограничения на ресурсы
          ▪ cpus              | Ограничения на CPU
          ▪ memory            | Ограничения на память
        ▪ reservations:       | Резервация ресурсов
          ▪ cpus: '0.0001'    | Резервация ресурсов CPU
          ▪ memory: 20M       | Резервация ресурсов памяти

      ▪ Пример

          deploy:
            resources:
              limits:
                cpus: '0.001'
                memory: 50M
              reservations:
                cpus: '0.0001'
                memory: 20M

    • restart_policy

      ▪ Описание
        - Настройка, нужно ли и как перезапускать контейнеры, если они завершают работу.
        - Заменяет опцию restart при работе через docker stack.

      ▪ Подопции

        ▪ condition           | [default] none / on-failure / any
        ▪ delay               | [0] Как долго ждать между попытками рестарта
        ▪ max_attempts        | [never give up] MAX число попыток рестарта
        ▪ window              | [decide immediately] Как долго ждать, прежде чем решить, что рестарт удался

      ▪ Пример

          deploy:
            restart_policy:
              condition: on-failure
              delay: 5s
              max_attempts: 3
              window: 120s

    • labels

      ▪ Описание
        - Назначить ярлыки для сервиса.
        - Потом по ярлыкам можно назначать ограничения на размещение
          в placement.

      ▪ ВНИМАНИЕ!
        - Ярлыки приклеиваются К СЕРВИСА, а НЕ К КОНТЕЙНЕРАМ.
        - Чтобы их приклеить к контейнерам, используй label вне ключа deploy.

      ▪ Примеры

        ▪ Приклеить ярлык к сервису

          version: "3"
          services:
            web:
              image: web
              deploy:
                labels:
                  com.example.description: "This label will appear on the web service"

        ▪ Приклеить ярлык к контейнеру

          version: "3"
          services:
            web:
              image: web
              labels:
                com.example.description: "This label will appear on all containers for the web service"

  > Туториал по деплою compose-сервиса в swarm

    1. Подготовим compose-файл
      - Укажем ограничение, деплой только на узел-менеджер.
      - Файл:

        version: "3"
        services:

          proxy:
            image: vfarcic/docker-flow-proxy
            ports:
              - 80:80
              - 443:443
            networks:
              - proxy
            environment:
              - LISTENER_ADDRESS=swarm-listener
              - MODE=swarm
            deploy:
              replicas: 2

          swarm-listener:
            image: vfarcic/docker-flow-swarm-listener
            networks:
              - proxy
            volumes:
              - /var/run/docker.sock:/var/run/docker.sock
            environment:
              - DF_NOTIFY_CREATE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/reconfigure
              - DF_NOTIFY_REMOVE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/remove
            deploy:
              placement:
                constraints: [node.role == manager]

        networks:
          proxy:
            external: true

    2. Задеплоим его на менеджера
      - Создадим новый стэк, назовём его proxy.
      - Compose-файл "docker-compose-stack.yml" у нас в текущем каталоге.
      - Задеплоим:

          docker stack deploy -c docker-compose-stack.yml proxy

    3. Проверим, создался ли стэк, и число задач в нём
      - Для этого выполним:

          docker stack ls

    4. Проверим состояние контейнеров стека
      - Для этого выполним:

          docker stack ps proxy














































- Будет один менеджер, и много воркеров.
- У всех будет установлен Docker, Compose, Infinit.
- На менеджере будет папочка с проектом, которая
  будет иметь отдельный github-репозиторий.
- Менеджер будет использовать CLI-команды для запуска проекта на воркерах.
- На менеджере будет папка для постоянных данных приложений,
  запущенных на воркерах.
- При формировании compose-файла при запуске очередного приложения на воркере,
  туда будет пробрасываться соотв.папка с постоянными данными.
  Внутри docker-контейнеров эта папка всегда будет называться одинаково.
- Stateless же части приложения будут интегрированы в docker-образы
  с помощью COPY.
- На локалке вместо docker deploy будет применяться docker compose,
  локальные конфиги, и пробрасываться будут данные из локальной
  папки, а не из удалённой.
- Кроме того, на локалке будут лежать локальные .git репозитории
  всех docker-образов. И при локальной отладке, в docker-compose
  в контейнеры будет пробрасываться соотв.код из этих образов,
  что позволит отлаживать всё в реалтайм, как в старой версии проекта.






    • Stateless/Stateful приложения

      ▪ Stateless приложения
        - Это приложения, не имеющие состояния.
        - После выключения никаких данных не сохраняется.
        - Перезагрузка сбрасывает состояние такого приложения к исходному.
        - Например, такое приложение может использовать в качестве хранилища
          данных Redis, но у которого отключено постоянное хранение данных.
        - А вот, например, MySQL такое приложение использовать не может,
          ведь для него надо постоянно хранить данные, которые должны
          сохраняться даже после выключения и перезагрузки приложения.

      ▪ Stateful приложения
        - Это приложения, имеющие и сохраняющие состояние.
        - Его можно включать/выключать/перезагружать, и при очередном
          включении приложение будет находиться в том же состоянии,
          что перед предыдущим выключением.
        - Чтобы приложение было stateful, у него должна быть возможность
          где-то хранить данные о своём состоянии на постоянной основе.
        - Например, используя MySQL. Состояние приложения будет храниться
          в базах данных, которые никуда не исчезают даже после выключения
          приложения.

    • ECS vs VPS

      ▪ Управление большим количеством приложений на docker
        - Если у вас лишь 1-но приложение, то обычный VPS вполней сойдёт.
        - Вы его настроите 1 раз, запустите приложение, будете за ним следить.
        - Но что, если вам надо контролировать 10/100/1000-чи docker-приложений?
        - Задача на обычных VPS становится невыполнимой с ростом числа приложений.
        - Тут на помощь приходит Amazon EC2 Container Service.
        - Он берёт на себя настройку, масштабирование, мониторинг.
        - И предоставляет API для управления всем этим делом.
        - Так что, можно написать себе центр управления, и управлять через API.

      ▪ Об Amazon EC2 Container Service (ECS)

        Amazon EC2 Container Service – это высокопроизводительный сервис
        управления контейнерами с высокими возможностями масштабирования.
        Он поддерживает контейнеры Docker и позволяет с легкостью запускать
        приложения в автоматически управляемом кластере инстансов Amazon EC2.
        C Amazon ECS вам не нужно будет самостоятельно устанавливать,
        масштабировать и обслуживать инфраструктуру управления кластерами.
        С помощью простых вызовов API вы сможете запускать и останавливать
        контейнерные приложения Docker, получать данные о состоянии всего
        кластера и пользоваться многими привычными возможностями, например
        группами безопасности, Elastic Load Balancing, томами EBS и ролями IAM.
        Используя сервис Amazon ECS, можно запланировать размещение контейнеров
        в вашем кластере с учетом потребности в ресурсах и требований к
        доступности. Вы также можете интегрировать собственный планировщик
        или планировщики сторонних разработчиков с учетом конкретных
        требований, связанных с вашим бизнесом или приложениями.

      ▪ Stateful-приложения без Infinit (или аналога) на ECS не запустить
        - При запуске приложения на VPS, вы пробрасываете данные в контейнеры.
        - Но ECS просто запускает контейнеры, данные пробрасывать неоткуда.
        - Поэтому, без использования Infinit, stateful-приложения на ECS в пролёте.

    • Упаковка приложения в docker-образ

      ▪ Упаковка stateless-приложения
        - Упаковать оное в docker-образ легко.
        - Просто кладём код с приложением в папку образа, рядом с DockerFile.
        - А в DockerFile копируем код приложения с помощью COPY в котейнер.
        - Для запуска такого приложения достаточно просто запустить котейнер из образа.
        - Это можно сделать, в том числе, и на Amazon Container Service.

      ▪ Упаковка stateful-приложения
        - Ранее это было либо невозможной, либо очень сложной задачей.
        - Ведь контейнеры не могут хранить никакую постоянную информацию.
        - При запуске приложения контейнер создаётся, а при остановке уничтожается,
          вместе со всей накопленной внутри него за время работы информацией.
        - Но теперь Infinit даёт возможность использовать внешнее хранилище,
          которое находится на удалённом сервера, например, в Amazon S3,
          либо на каком-нибудь VPS.
        - Мы упаковываем stateless-часть приложения в docker-образы,
          а данные от stateful-части храним в удалённом хранилище.
        - При запуске контейнера, через плагин Infinit для docker,
          мы пробрасываем каталог из удалённого хранилища в контейнер,
          и получаем stateful-приложение.

  > Задача данного руководства

      Разработать и реализовать на практике архитектуру stateful-приложения,
      в качестве хостинга использующее Amazon EC2 Container Service,
      stateless-часть которого полностью упакована в docker-образы,
      данные stateful-части которого хранятся в Amazon S3,
      и копии которого можно создавать за несколько минут.


А2. Infinit Storage Platform

  --------------------------------------
  Подоглавление:

    # Ссылки
    # Вводная информация об Infinit Storage Platform

      ▪ Open-source, бесплатно
      ▪ Децентрализованное хранилище
      ▪ Особенности Infinit
        ▪ Это софт
        ▪ CLI и API
        ▪ Масштабируемость
        ▪ Самоорганизация
        ▪ Многозадачность
        ▪ Современность
      ▪ Интеграция с Docker

    #



  --------------------------------------

  > Ссылки
    - См.ссылки в начале данного руководства.

  > Вводная информация об Infinit Storage Platform

    • Open-source, бесплатно
      - На данный момент (03.05.2017) Infinit является open-source проектом.
      - И пользоваться и можно невозбранно и бесплатно.
      - Однако, планируется плантая Enterprise-версия с доп.функционалом.
      - Но для целей нашего руководства должно хватить и бесплатной версии.

    • Децентрализованное хранилище
      - Infinit, это софт для организации децентрализованного хранилища данных.
      - Из коробки в community edition поддерживает 3 интерфейса:

        ▪ [block] NBD (Network Block Device)
        ▪ [object] Amazon S3
        ▪ [file] FUSE & Dokan

    • Особенности Infinit

      ▪ Это софт
        - Infinit, это софт, его можно развернуть где угодно.
        - На железке, виртуалке, в контейнере, и т.д.

      ▪ CLI и API
        - Работать с Infinit можно через CLI и API.

      ▪ Масштабируемость
        - Infinit может объединять сколько угодно узлов.
        - И таким образом, можно быстро масштабировать хранилище.

      ▪ Самоорганизация
        - В Infinit действует механизм отказоустойчивости.

      ▪ Много интерфейсов
        - Доступ к данным в Infinit можно получить через много интерфейсов.
        - В community-версии доступны описанные выше интерфейсы.

      ▪ Современность
        - В Infinit встроено много дополнительной полезной функциональности.
        - Такой, как: шифрование, распределённое кэширование, многоузловое резервирование,
          hyper-convergence и multi-tiering.

    • Интеграция с Docker
      - В начале 2017-го Docker купил Infinit.
      - И, поэтому, активно интегрирует его в себя.
      - Подробнее о текущих возможностях в одной из глав ниже.


А3. Amazon EC2 Container Service (ECS)

  --------------------------------------
  Подоглавление:

    # Ссылки
    #



  --------------------------------------

  > Ссылки









Запрос в google: "Amazon Container Service s3 volume"
Amazon Elastic File System
Using Amazon EFS to Persist Data from Amazon ECS Containers
  https://aws.amazon.com/ru/blogs/compute/using-amazon-efs-to-persist-data-from-amazon-ecs-containers/


AWS CloudFormation
https://aws.amazon.com/ru/cloudformation/aws-cloudformation-templates/





Какова должна быть структура приложения?

  • Stateless-часть должна быть чётко отделена от Stateful-части.
  • Stateless-часть должна быть интегрирована в соотв.контейнеры.
  • Stateful-часть должна лежать

Что должно входить в stateless-часть

  • Весь код приложения должен быть частью docker-образов
    - Чтобы не было больше проблем с обновлением.
    - Просто обновил docker-образ и всё.

Что должно входить в stateful-часть

  • Базы данных redis и mysql.
  •




Что мы имеем сейчас
    -






Короче говоря

  • Что касается постоянного хранения данных

    1. Infinit позволяет подключить S3 в папочку на VDS.
    2. В этой папочке можно хранить всю stateful-часть приложения.
    3. Папочку можно пробрасывать в контейнеры.

  • Как быстро и автоматически создать клон сервиса

    - Создаём в Amazon S3 bucket папку для нового экземпляра приложения,
      и заливаем туда шаблон stateful-части приложения.
    - Работаем с digital ocean.
    - Через API создаём новый дроплет.
    - Одновременно передаём туда SSH-ключи.
    - Подключаемся к нему, заливаем провижн-скрипт.
    - Последний устанавливает docker, docker-compose,
      Infinit, подключает S3-хранилище из amazon,
      выполняет другие задачи.
    - Подключаем созданный дроплет к рою через мастера.
    - Мастером отдаём приказ запустить конкретно на
      этом узле docker-compose приложение.

  • GUI для управления docker swarm

    1. http://portainer.io/
    2. https://shipyard-project.com/



Как запускать compose-приложения в определённом ноде swarm'а


Использование docker-compose вместе со swarm
http://docs.master.dockerproject.org/compose/swarm/
http://blog.terranillius.com/post/composev3_swarm/



Я хочу

  1. Создать дроплет.
  2. Выполнить провижн-скрипт.
  3. Пробросить в дроплет папочку из S3.
  4. Подключить дроплет к своему Swarm.
  5. Запустить на дроплете compose-приложение.
  ???
  PROFIT

Как такое приложение будет разрабатываться на локалке

  1. На linux с установленными docker, compose.



Что нужно, чтобы я мог за 5 минут деплоить клоны приложения,
отслеживать статус, легко обновлять, легко разрабатывать на локалке?

  1. Возможность запускать приложение командой
     docker deploy --compose-file docker-compose.yml myapp
    - Придётся избавиться от up и cli на воркерах вообще.
    - Поскольку через docker swarm никакой cli не выполнить.

  2.






План таков
- Будет один менеджер, и много воркеров.
- У всех будет установлен Docker, Compose, Infinit.
- На менеджере будет папочка с проектом, которая
  будет иметь отдельный github-репозиторий.
- Менеджер будет использовать CLI-команды для запуска проекта на воркерах.
- На менеджере будет папка для постоянных данных приложений,
  запущенных на воркерах.
- При формировании compose-файла при запуске очередного приложения на воркере,
  туда будет пробрасываться соотв.папка с постоянными данными.
  Внутри docker-контейнеров эта папка всегда будет называться одинаково.
- Stateless же части приложения будут интегрированы в docker-образы
  с помощью COPY.
- На локалке вместо docker deploy будет применяться docker compose,
  локальные конфиги, и пробрасываться будут данные из локальной
  папки, а не из удалённой.
- Кроме того, на локалке будут лежать локальные .git репозитории
  всех docker-образов. И при локальной отладке, в docker-compose
  в контейнеры будет пробрасываться соотв.код из этих образов,
  что позволит отлаживать всё в реалтайм, как в старой версии проекта.

В конечном счёте
- Надо будет купить дроплет для менеджера,
  выполнить в нём провижн-скрипт менеджера,
  склонировать туда проект, купить домен
  steam-solutions.com, поднять лендос,
  админку, наделать в админке интерфейсов,
  в т.ч. возможность создать клон проекта
  нажатием одной кнопки.
- При поступлении заказа на клон проекта,
  надо будет купить дроплет для воркера,
  выполнить в нём провинжн-скрипт воркера,
  указав токен для подключения к рою менеджера,
  убедиться в наличии Infinit volume с менеджера,
  на менеджере выполнить . app up приложения на воркере,
  убедиться, что приложение запустилось и работает.



